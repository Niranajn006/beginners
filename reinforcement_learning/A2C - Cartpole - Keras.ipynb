{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
    "class A2C:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./save_model/cartpole_actor.h5\")\n",
    "            self.critic.load_weights(\"./save_model/cartpole_critic.h5\")\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # make A2C agent\n",
    "    agent = A2C(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500.0 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_a2c.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    sys.exit()\n",
    "\n",
    "        # save the model\n",
    "        if e % 50 == 0:\n",
    "            agent.actor.save_weights(\"./save_model/cartpole_actor.h5\")\n",
    "            agent.critic.save_weights(\"./save_model/cartpole_critic.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
